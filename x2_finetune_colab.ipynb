{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "2afb1bba",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: timm in /usr/local/lib/python3.12/dist-packages (1.0.22)\n",
            "Requirement already satisfied: diffusers in /usr/local/lib/python3.12/dist-packages (0.35.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from timm) (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from timm) (0.24.0+cu126)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from timm) (6.0.3)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (from timm) (0.36.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from timm) (0.7.0)\n",
            "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.12/dist-packages (from diffusers) (8.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from diffusers) (3.20.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from diffusers) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from diffusers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from diffusers) (2.32.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from diffusers) (11.3.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (25.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (1.2.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib_metadata->diffusers) (3.23.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->diffusers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->diffusers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->diffusers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->diffusers) (2025.11.12)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->timm) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->timm) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->timm) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "# Install dependencies\n",
        "!pip install timm diffusers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "7559cfd9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'DiT'...\n",
            "remote: Enumerating objects: 102, done.\u001b[K\n",
            "remote: Total 102 (delta 0), reused 0 (delta 0), pack-reused 102 (from 1)\u001b[K\n",
            "Receiving objects: 100% (102/102), 6.37 MiB | 56.19 MiB/s, done.\n",
            "Resolving deltas: 100% (55/55), done.\n",
            "/content/DiT/DiT\n"
          ]
        }
      ],
      "source": [
        "# IMPORTANT: This notebook is designed for YOUR CUSTOM DiT fork with x2 modifications.\n",
        "# \n",
        "# Option 1: If you've pushed your code to GitHub, replace the URL below:\n",
        "# !git clone https://github.com/YOUR_USERNAME/YOUR_REPO.git\n",
        "# %cd YOUR_REPO\n",
        "#\n",
        "# Option 2: For now, we clone the base DiT repo and overwrite with your custom files:\n",
        "!git clone https://github.com/facebookresearch/DiT.git\n",
        "%cd DiT\n",
        "\n",
        "# The next cells will overwrite models.py (with x2 modifications) and add new scripts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "bacb1adb",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting models.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile models.py\n",
        "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
        "# All rights reserved.\n",
        "\n",
        "# This source code is licensed under the license found in the\n",
        "# LICENSE file in the root directory of this source tree.\n",
        "# --------------------------------------------------------\n",
        "# References:\n",
        "# GLIDE: https://github.com/openai/glide-text2im\n",
        "# MAE: https://github.com/facebookresearch/mae/blob/main/models_mae.py\n",
        "# --------------------------------------------------------\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import math\n",
        "import timm\n",
        "from timm.models.vision_transformer import PatchEmbed, Attention, Mlp, VisionTransformer, Block\n",
        "\n",
        "\n",
        "def modulate(x, shift, scale):\n",
        "    return x * (1 + scale.unsqueeze(1)) + shift.unsqueeze(1)\n",
        "\n",
        "\n",
        "#################################################################################\n",
        "#               Embedding Layers for Timesteps and Class Labels                 #\n",
        "#################################################################################\n",
        "\n",
        "class TimestepEmbedder(nn.Module):\n",
        "    \"\"\"\n",
        "    Embeds scalar timesteps into vector representations.\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_size, frequency_embedding_size=256):\n",
        "        super().__init__()\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(frequency_embedding_size, hidden_size, bias=True),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(hidden_size, hidden_size, bias=True),\n",
        "        )\n",
        "        self.frequency_embedding_size = frequency_embedding_size\n",
        "\n",
        "    @staticmethod\n",
        "    def timestep_embedding(t, dim, max_period=10000):\n",
        "        \"\"\"\n",
        "        Create sinusoidal timestep embeddings.\n",
        "        :param t: a 1-D Tensor of N indices, one per batch element.\n",
        "                          These may be fractional.\n",
        "        :param dim: the dimension of the output.\n",
        "        :param max_period: controls the minimum frequency of the embeddings.\n",
        "        :return: an (N, D) Tensor of positional embeddings.\n",
        "        \"\"\"\n",
        "        # https://github.com/openai/glide-text2im/blob/main/glide_text2im/nn.py\n",
        "        half = dim // 2\n",
        "        freqs = torch.exp(\n",
        "            -math.log(max_period) * torch.arange(start=0, end=half, dtype=torch.float32) / half\n",
        "        ).to(device=t.device)\n",
        "        args = t[:, None].float() * freqs[None]\n",
        "        embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n",
        "        if dim % 2:\n",
        "            embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)\n",
        "        return embedding\n",
        "\n",
        "    def forward(self, t):\n",
        "        t_freq = self.timestep_embedding(t, self.frequency_embedding_size)\n",
        "        t_emb = self.mlp(t_freq)\n",
        "        return t_emb\n",
        "\n",
        "\n",
        "class LabelEmbedder(nn.Module):\n",
        "    \"\"\"\n",
        "    Embeds class labels into vector representations. Also handles label dropout for classifier-free guidance.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes, hidden_size, dropout_prob):\n",
        "        super().__init__()\n",
        "        use_cfg_embedding = dropout_prob > 0\n",
        "        self.embedding_table = nn.Embedding(num_classes + use_cfg_embedding, hidden_size)\n",
        "        self.num_classes = num_classes\n",
        "        self.dropout_prob = dropout_prob\n",
        "\n",
        "    def token_drop(self, labels, force_drop_ids=None):\n",
        "        \"\"\"\n",
        "        Drops labels to enable classifier-free guidance.\n",
        "        \"\"\"\n",
        "        if force_drop_ids is None:\n",
        "            drop_ids = torch.rand(labels.shape[0], device=labels.device) < self.dropout_prob\n",
        "        else:\n",
        "            drop_ids = force_drop_ids == 1\n",
        "        labels = torch.where(drop_ids, self.num_classes, labels)\n",
        "        return labels\n",
        "\n",
        "    def forward(self, labels, train, force_drop_ids=None):\n",
        "        use_dropout = self.dropout_prob > 0\n",
        "        if (train and use_dropout) or (force_drop_ids is not None):\n",
        "            labels = self.token_drop(labels, force_drop_ids)\n",
        "        embeddings = self.embedding_table(labels)\n",
        "        return embeddings\n",
        "\n",
        "\n",
        "#################################################################################\n",
        "#                                 Core DiT Model                                #\n",
        "#################################################################################\n",
        "\n",
        "class DiTBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    A DiT block with adaptive layer norm zero (adaLN-Zero) conditioning.\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_size, num_heads, mlp_ratio=4.0, **block_kwargs):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(hidden_size, elementwise_affine=False, eps=1e-6)\n",
        "        self.attn = Attention(hidden_size, num_heads=num_heads, qkv_bias=True, **block_kwargs)\n",
        "        self.norm2 = nn.LayerNorm(hidden_size, elementwise_affine=False, eps=1e-6)\n",
        "        mlp_hidden_dim = int(hidden_size * mlp_ratio)\n",
        "        approx_gelu = lambda: nn.GELU(approximate=\"tanh\")\n",
        "        self.mlp = Mlp(in_features=hidden_size, hidden_features=mlp_hidden_dim, act_layer=approx_gelu, drop=0)\n",
        "        self.adaLN_modulation = nn.Sequential(\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(hidden_size, 6 * hidden_size, bias=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, c):\n",
        "        shift_msa, scale_msa, gate_msa, shift_mlp, scale_mlp, gate_mlp = self.adaLN_modulation(c).chunk(6, dim=1)\n",
        "        x = x + gate_msa.unsqueeze(1) * self.attn(modulate(self.norm1(x), shift_msa, scale_msa))\n",
        "        x = x + gate_mlp.unsqueeze(1) * self.mlp(modulate(self.norm2(x), shift_mlp, scale_mlp))\n",
        "        return x\n",
        "\n",
        "\n",
        "class FinalLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    The final layer of DiT.\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_size, patch_size, out_channels):\n",
        "        super().__init__()\n",
        "        self.norm_final = nn.LayerNorm(hidden_size, elementwise_affine=False, eps=1e-6)\n",
        "        self.linear = nn.Linear(hidden_size, patch_size * patch_size * out_channels, bias=True)\n",
        "        self.adaLN_modulation = nn.Sequential(\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(hidden_size, 2 * hidden_size, bias=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, c):\n",
        "        shift, scale = self.adaLN_modulation(c).chunk(2, dim=1)\n",
        "        x = modulate(self.norm_final(x), shift, scale)\n",
        "        x = self.linear(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class DiT(nn.Module):\n",
        "    \"\"\"\n",
        "    Diffusion model with a Transformer backbone.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size=32,\n",
        "        patch_size=2,\n",
        "        in_channels=4,\n",
        "        hidden_size=1152,\n",
        "        depth=28,\n",
        "        num_heads=16,\n",
        "        mlp_ratio=4.0,\n",
        "        class_dropout_prob=0.1,\n",
        "        num_classes=1000,\n",
        "        learn_sigma=True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.learn_sigma = learn_sigma\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = in_channels * 2 if learn_sigma else in_channels\n",
        "        self.patch_size = patch_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        self.x_embedder = PatchEmbed(input_size, patch_size, in_channels, hidden_size, bias=True)\n",
        "        self.x2_embedder = PatchEmbed(input_size, patch_size * 2, in_channels, hidden_size, bias=True)\n",
        "        self.t_embedder = TimestepEmbedder(hidden_size)\n",
        "        self.y_embedder = LabelEmbedder(num_classes, hidden_size, class_dropout_prob)\n",
        "        num_patches = self.x_embedder.num_patches\n",
        "        # Will use fixed sin-cos embedding:\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, hidden_size), requires_grad=False)\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            DiTBlock(hidden_size, num_heads, mlp_ratio=mlp_ratio) for _ in range(depth)\n",
        "        ])\n",
        "        # ViT block for processing x2\n",
        "        # Will be initialized with target dimensions, but may be replaced with pretrained block if dimensions differ\n",
        "        self.x2_vit_block = Block(\n",
        "            dim=hidden_size,\n",
        "            num_heads=num_heads,\n",
        "            mlp_ratio=mlp_ratio,\n",
        "            qkv_bias=True\n",
        "        )\n",
        "        # Projection layers for adapting dimensions if needed (will be created if dimensions don't match)\n",
        "        self.x2_vit_proj_in = None\n",
        "        self.x2_vit_proj_out = None\n",
        "        self.final_layer = FinalLayer(hidden_size, patch_size, self.out_channels)\n",
        "        self.initialize_weights()\n",
        "\n",
        "    def initialize_weights(self):\n",
        "        # Initialize transformer layers:\n",
        "        def _basic_init(module):\n",
        "            if isinstance(module, nn.Linear):\n",
        "                torch.nn.init.xavier_uniform_(module.weight)\n",
        "                if module.bias is not None:\n",
        "                    nn.init.constant_(module.bias, 0)\n",
        "        self.apply(_basic_init)\n",
        "\n",
        "        # Initialize (and freeze) pos_embed by sin-cos embedding:\n",
        "        pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1], int(self.x_embedder.num_patches ** 0.5))\n",
        "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
        "\n",
        "        # Initialize patch_embed like nn.Linear (instead of nn.Conv2d):\n",
        "        w = self.x_embedder.proj.weight.data\n",
        "        nn.init.xavier_uniform_(w.view([w.shape[0], -1]))\n",
        "        nn.init.constant_(self.x_embedder.proj.bias, 0)\n",
        "\n",
        "        # Initialize label embedding table:\n",
        "        nn.init.normal_(self.y_embedder.embedding_table.weight, std=0.02)\n",
        "\n",
        "        # Initialize timestep embedding MLP:\n",
        "        nn.init.normal_(self.t_embedder.mlp[0].weight, std=0.02)\n",
        "        nn.init.normal_(self.t_embedder.mlp[2].weight, std=0.02)\n",
        "\n",
        "        # Zero-out adaLN modulation layers in DiT blocks:\n",
        "        for block in self.blocks:\n",
        "            nn.init.constant_(block.adaLN_modulation[-1].weight, 0)\n",
        "            nn.init.constant_(block.adaLN_modulation[-1].bias, 0)\n",
        "\n",
        "        # Zero-out output layers:\n",
        "        nn.init.constant_(self.final_layer.adaLN_modulation[-1].weight, 0)\n",
        "        nn.init.constant_(self.final_layer.adaLN_modulation[-1].bias, 0)\n",
        "        nn.init.constant_(self.final_layer.linear.weight, 0)\n",
        "        nn.init.constant_(self.final_layer.linear.bias, 0)\n",
        "        \n",
        "        # Load pre-trained timm ViT weights for x2_vit_block\n",
        "        self.load_pretrained_vit_weights()\n",
        "\n",
        "    def load_pretrained_vit_weights(self, vit_model_name='vit_large_patch16_224'):\n",
        "        \"\"\"\n",
        "        Load pre-trained timm ViT weights for x2_vit_block.\n",
        "        If dimensions don't match, uses projection layers to adapt.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Load pre-trained ViT model\n",
        "            print(f\"[DiT] Loading pre-trained ViT model: {vit_model_name}\", flush=True)\n",
        "            pretrained_vit = timm.create_model(vit_model_name, pretrained=True)\n",
        "            pretrained_vit.eval()\n",
        "            \n",
        "            # Extract the last block\n",
        "            pretrained_block = pretrained_vit.blocks[-1]\n",
        "            pretrained_dim = pretrained_vit.embed_dim\n",
        "            # Get num_heads from the attention module or infer from qkv weight shape\n",
        "            pretrained_num_heads = None\n",
        "            # Try multiple ways to get num_heads\n",
        "            if hasattr(pretrained_block.attn, 'num_heads'):\n",
        "                pretrained_num_heads = pretrained_block.attn.num_heads\n",
        "            elif hasattr(pretrained_vit, 'num_heads'):\n",
        "                pretrained_num_heads = pretrained_vit.num_heads\n",
        "            else:\n",
        "                # Infer from qkv weight shape\n",
        "                # qkv weight shape is (3 * num_heads * head_dim, embed_dim)\n",
        "                # where embed_dim = num_heads * head_dim\n",
        "                # So: qkv_out_dim = 3 * embed_dim, and head_dim = embed_dim / num_heads\n",
        "                # Therefore: qkv_out_dim = 3 * num_heads * (embed_dim / num_heads) = 3 * embed_dim\n",
        "                # This means we can't directly get num_heads from qkv_out_dim alone\n",
        "                # But we can calculate: num_heads = embed_dim / head_dim\n",
        "                # And head_dim = qkv_out_dim / (3 * num_heads) = embed_dim / num_heads\n",
        "                # So: qkv_out_dim = 3 * embed_dim (always true for standard ViT)\n",
        "                # We need to infer head_dim. Standard head_dim values: 64 (most common)\n",
        "                qkv_out_dim = pretrained_block.attn.qkv.weight.shape[0]\n",
        "                # Calculate head_dim from qkv: head_dim = qkv_out_dim / (3 * num_heads)\n",
        "                # Since we don't know num_heads, let's use common defaults\n",
        "                # Common ViT configs: embed_dim 768 -> 12 heads (head_dim=64), 1024 -> 16 heads (head_dim=64)\n",
        "                if pretrained_dim == 768:\n",
        "                    pretrained_num_heads = 12\n",
        "                elif pretrained_dim == 1024:\n",
        "                    pretrained_num_heads = 16\n",
        "                elif pretrained_dim == 1280:\n",
        "                    pretrained_num_heads = 16\n",
        "                else:\n",
        "                    # Calculate: assume head_dim = 64 (most common)\n",
        "                    pretrained_num_heads = pretrained_dim // 64\n",
        "                    if pretrained_num_heads <= 0 or pretrained_num_heads > 32:\n",
        "                        pretrained_num_heads = 16  # fallback to common value\n",
        "                print(f\"[DiT] Inferred num_heads={pretrained_num_heads} from embed_dim={pretrained_dim}\", flush=True)\n",
        "            \n",
        "            print(f\"[DiT] Pre-trained ViT block: dim={pretrained_dim}, num_heads={pretrained_num_heads}\", flush=True)\n",
        "            print(f\"[DiT] Target x2_vit_block: dim={self.x2_vit_block.norm1.normalized_shape[0]}, num_heads={self.num_heads}\", flush=True)\n",
        "            \n",
        "            # Check if dimensions match\n",
        "            if pretrained_dim == self.hidden_size and pretrained_num_heads == self.num_heads:\n",
        "                # Direct weight loading if dimensions match\n",
        "                print(f\"[DiT] Dimensions match! Loading weights directly...\", flush=True)\n",
        "                self.x2_vit_block.load_state_dict(pretrained_block.state_dict(), strict=True)\n",
        "                print(f\"[DiT] ✓ Successfully loaded pre-trained ViT weights for x2_vit_block\", flush=True)\n",
        "            else:\n",
        "                # Dimensions don't match - replace block with pretrained dimensions and use projection layers\n",
        "                print(f\"[DiT] Dimensions don't match. Creating block with pretrained dimensions and projection layers...\", flush=True)\n",
        "                \n",
        "                # Create a new block with pretrained dimensions\n",
        "                pretrained_mlp_ratio = pretrained_vit.mlp_ratio if hasattr(pretrained_vit, 'mlp_ratio') else 4.0\n",
        "                self.x2_vit_block = Block(\n",
        "                    dim=pretrained_dim,\n",
        "                    num_heads=pretrained_num_heads,\n",
        "                    mlp_ratio=pretrained_mlp_ratio,\n",
        "                    qkv_bias=True\n",
        "                )\n",
        "                \n",
        "                # Load pre-trained weights into the new block\n",
        "                self.x2_vit_block.load_state_dict(pretrained_block.state_dict(), strict=True)\n",
        "                \n",
        "                # Create projection layers\n",
        "                self.x2_vit_proj_in = nn.Linear(self.hidden_size, pretrained_dim)\n",
        "                self.x2_vit_proj_out = nn.Linear(pretrained_dim, self.hidden_size)\n",
        "                # Initialize projection layers\n",
        "                nn.init.xavier_uniform_(self.x2_vit_proj_in.weight)\n",
        "                nn.init.constant_(self.x2_vit_proj_in.bias, 0)\n",
        "                nn.init.xavier_uniform_(self.x2_vit_proj_out.weight)\n",
        "                nn.init.constant_(self.x2_vit_proj_out.bias, 0)\n",
        "                \n",
        "                print(f\"[DiT] ✓ Successfully loaded pre-trained ViT weights into block with dim={pretrained_dim}\", flush=True)\n",
        "                print(f\"[DiT] Using projection layers to adapt dimensions: {self.hidden_size} -> {pretrained_dim} -> {self.hidden_size}\", flush=True)\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"[DiT] Warning: Could not load pre-trained ViT weights: {e}\", flush=True)\n",
        "            print(f\"[DiT] Using randomly initialized weights for x2_vit_block\", flush=True)\n",
        "\n",
        "    def unpatchify(self, x):\n",
        "        \"\"\"\n",
        "        x: (N, T, patch_size**2 * C)\n",
        "        imgs: (N, H, W, C)\n",
        "        \"\"\"\n",
        "        c = self.out_channels\n",
        "        p = self.x_embedder.patch_size[0]\n",
        "        h = w = int(x.shape[1] ** 0.5)\n",
        "        assert h * w == x.shape[1]\n",
        "\n",
        "        x = x.reshape(shape=(x.shape[0], h, w, p, p, c))\n",
        "        x = torch.einsum('nhwpqc->nchpwq', x)\n",
        "        imgs = x.reshape(shape=(x.shape[0], c, h * p, h * p))\n",
        "        return imgs\n",
        "\n",
        "    def forward(self, x, t, y):\n",
        "        \"\"\"\n",
        "        Forward pass of DiT.\n",
        "        x: (N, C, H, W) tensor of spatial inputs (images or latent representations of images)\n",
        "        t: (N,) tensor of diffusion timesteps\n",
        "        y: (N,) tensor of class labels\n",
        "        \"\"\"\n",
        "        # Log forward pass execution\n",
        "        print(f\"[DiT Forward] Batch: {x.shape}, Timesteps: [{t.min().item():.0f}-{t.max().item():.0f}], Classes: {y[:min(4,len(y))].tolist()}\", flush=True)\n",
        "        \n",
        "        # skip = self.x_embedder(x)                                 # preserve pre-block representation\n",
        "        x2 = self.x2_embedder(x)\n",
        "        # print(f\"[DiT Forward] skip: {skip.shape}\", flush=True)\n",
        "        x = self.x_embedder(x) + self.pos_embed  # (N, T, D), where T = H * W / patch_size ** 2\n",
        "        t = self.t_embedder(t)                   # (N, D)\n",
        "        y = self.y_embedder(y, self.training)    # (N, D)\n",
        "        c = t + y                                # (N, D)\n",
        "        # Pool x2 from shape (N, 4T, D) to (N, T, D) to match x\n",
        "        # Need to transpose for avg_pool1d which expects (N, C, L) format\n",
        "        x2 = x2.transpose(1, 2)  # (N, D, 4T)\n",
        "        print(f\"[DiT Forward] x2 after transpose: {x2.shape}\", flush=True)\n",
        "        # x2 = torch.avg_pool1d(x2, kernel_size=4, stride=4)  # (N, D, T)\n",
        "        # print(f\"[DiT Forward] x2 after AvgPooling: {x2.shape}\", flush=True)\n",
        "        x2 = torch.nn.functional.interpolate(x2, size=256, mode='linear', align_corners=False)\n",
        "        print(f\"[DiT Forward] x2 after interpolate: {x2.shape}\", flush=True)\n",
        "        x2 = x2.transpose(1, 2)  # (N, T, D)\n",
        "        print(f\"[DiT Forward] x2 after transpose back: {x2.shape}\", flush=True)\n",
        "        # Apply ViT block to x2 to ensure same processing as x\n",
        "        # Use projection layers if dimensions don't match\n",
        "        if self.x2_vit_proj_in is not None:\n",
        "            x2 = self.x2_vit_proj_in(x2)  # Project to pre-trained ViT dimension\n",
        "        x2 = self.x2_vit_block(x2)  # (N, T, D) - processed by pre-trained ViT block\n",
        "        if self.x2_vit_proj_out is not None:\n",
        "            x2 = self.x2_vit_proj_out(x2)  # Project back to hidden_size\n",
        "        print(f\"[DiT Forward] x2 after ViT block: {x2.shape}\", flush=True)\n",
        "        print(f\"[DiT Forward] x: {x.shape}\", flush=True)\n",
        "        print(f\"[DiT Forward] x after addition: {x.shape}\", flush=True)\n",
        "        for block in self.blocks:\n",
        "            x = block(x, c)\n",
        "            # if i == 0:\n",
        "            # x = x + x2\n",
        "        # for i, block in enumerate(self.blocks):\n",
        "        #     x = block(x, c)\n",
        "        #     if i == 0:\n",
        "        #         x = x + x2\n",
        "            # (N, T, D)\n",
        "        # x = x + skip                             # skip connection across all blocks\n",
        "        x = x + x2\n",
        "        \n",
        "        # print(f\"[DiT Forward] ✓ Skip org connection applied across {len(self.blocks)} blocks\", flush=True)\n",
        "        \n",
        "        x = self.final_layer(x, c)                # (N, T, patch_size ** 2 * out_channels)\n",
        "        x = self.unpatchify(x)                   # (N, out_channels, H, W)\n",
        "        return x\n",
        "\n",
        "    def forward_with_cfg(self, x, t, y, cfg_scale):\n",
        "        \"\"\"\n",
        "        Forward pass of DiT, but also batches the unconditional forward pass for classifier-free guidance.\n",
        "        \"\"\"\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"[CFG] Input: {x.shape}, CFG scale: {cfg_scale}\")\n",
        "        \n",
        "        # https://github.com/openai/glide-text2im/blob/main/notebooks/text2im.ipynb\n",
        "        half = x[: len(x) // 2]\n",
        "        combined = torch.cat([half, half], dim=0)\n",
        "        \n",
        "        model_out = self.forward(combined, t, y)\n",
        "        \n",
        "        # For exact reproducibility reasons, we apply classifier-free guidance on only\n",
        "        # three channels by default. The standard approach to cfg applies it to all channels.\n",
        "        # This can be done by uncommenting the following line and commenting-out the line following that.\n",
        "        # eps, rest = model_out[:, :self.in_channels], model_out[:, self.in_channels:]\n",
        "        eps, rest = model_out[:, :3], model_out[:, 3:]\n",
        "        cond_eps, uncond_eps = torch.split(eps, len(eps) // 2, dim=0)\n",
        "        half_eps = uncond_eps + cfg_scale * (cond_eps - uncond_eps)\n",
        "        eps = torch.cat([half_eps, half_eps], dim=0)\n",
        "        \n",
        "        print(f\"[CFG] ✓ Guidance applied, output: {torch.cat([eps, rest], dim=1).shape}\")\n",
        "        print(f\"{'='*60}\\n\")\n",
        "        \n",
        "        return torch.cat([eps, rest], dim=1)\n",
        "\n",
        "\n",
        "#################################################################################\n",
        "#                   Sine/Cosine Positional Embedding Functions                  #\n",
        "#################################################################################\n",
        "# https://github.com/facebookresearch/mae/blob/main/util/pos_embed.py\n",
        "\n",
        "def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False, extra_tokens=0):\n",
        "    \"\"\"\n",
        "    grid_size: int of the grid height and width\n",
        "    return:\n",
        "    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n",
        "    \"\"\"\n",
        "    grid_h = np.arange(grid_size, dtype=np.float32)\n",
        "    grid_w = np.arange(grid_size, dtype=np.float32)\n",
        "    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n",
        "    grid = np.stack(grid, axis=0)\n",
        "\n",
        "    grid = grid.reshape([2, 1, grid_size, grid_size])\n",
        "    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
        "    if cls_token and extra_tokens > 0:\n",
        "        pos_embed = np.concatenate([np.zeros([extra_tokens, embed_dim]), pos_embed], axis=0)\n",
        "    return pos_embed\n",
        "\n",
        "\n",
        "def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n",
        "    assert embed_dim % 2 == 0\n",
        "\n",
        "    # use half of dimensions to encode grid_h\n",
        "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n",
        "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n",
        "\n",
        "    emb = np.concatenate([emb_h, emb_w], axis=1) # (H*W, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n",
        "    \"\"\"\n",
        "    embed_dim: output dimension for each position\n",
        "    pos: a list of positions to be encoded: size (M,)\n",
        "    out: (M, D)\n",
        "    \"\"\"\n",
        "    assert embed_dim % 2 == 0\n",
        "    omega = np.arange(embed_dim // 2, dtype=np.float64)\n",
        "    omega /= embed_dim / 2.\n",
        "    omega = 1. / 10000**omega  # (D/2,)\n",
        "\n",
        "    pos = pos.reshape(-1)  # (M,)\n",
        "    out = np.einsum('m,d->md', pos, omega)  # (M, D/2), outer product\n",
        "\n",
        "    emb_sin = np.sin(out) # (M, D/2)\n",
        "    emb_cos = np.cos(out) # (M, D/2)\n",
        "\n",
        "    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "#################################################################################\n",
        "#                                   DiT Configs                                  #\n",
        "#################################################################################\n",
        "\n",
        "def DiT_XL_2(**kwargs):\n",
        "    print(\"Creating DiT-XL/2 model...\")\n",
        "    return DiT(depth=28, hidden_size=1152, patch_size=2, num_heads=16, **kwargs)\n",
        "\n",
        "def DiT_XL_4(**kwargs):\n",
        "    print(\"Creating DiT-XL/4 model...\")\n",
        "    return DiT(depth=28, hidden_size=1152, patch_size=4, num_heads=16, **kwargs)\n",
        "\n",
        "def DiT_XL_8(**kwargs):\n",
        "    return DiT(depth=28, hidden_size=1152, patch_size=8, num_heads=16, **kwargs)\n",
        "\n",
        "def DiT_L_2(**kwargs):\n",
        "    return DiT(depth=24, hidden_size=1024, patch_size=2, num_heads=16, **kwargs)\n",
        "\n",
        "def DiT_L_4(**kwargs):\n",
        "    return DiT(depth=24, hidden_size=1024, patch_size=4, num_heads=16, **kwargs)\n",
        "\n",
        "def DiT_L_8(**kwargs):\n",
        "    return DiT(depth=24, hidden_size=1024, patch_size=8, num_heads=16, **kwargs)\n",
        "\n",
        "def DiT_B_2(**kwargs):\n",
        "    return DiT(depth=12, hidden_size=768, patch_size=2, num_heads=12, **kwargs)\n",
        "\n",
        "def DiT_B_4(**kwargs):\n",
        "    return DiT(depth=12, hidden_size=768, patch_size=4, num_heads=12, **kwargs)\n",
        "\n",
        "def DiT_B_8(**kwargs):\n",
        "    return DiT(depth=12, hidden_size=768, patch_size=8, num_heads=12, **kwargs)\n",
        "\n",
        "def DiT_S_2(**kwargs):\n",
        "    return DiT(depth=12, hidden_size=384, patch_size=2, num_heads=6, **kwargs)\n",
        "\n",
        "def DiT_S_4(**kwargs):\n",
        "    return DiT(depth=12, hidden_size=384, patch_size=4, num_heads=6, **kwargs)\n",
        "\n",
        "def DiT_S_8(**kwargs):\n",
        "    return DiT(depth=12, hidden_size=384, patch_size=8, num_heads=6, **kwargs)\n",
        "\n",
        "\n",
        "DiT_models = {\n",
        "    'DiT-XL/2': DiT_XL_2,  'DiT-XL/4': DiT_XL_4,  'DiT-XL/8': DiT_XL_8,\n",
        "    'DiT-L/2':  DiT_L_2,   'DiT-L/4':  DiT_L_4,   'DiT-L/8':  DiT_L_8,\n",
        "    'DiT-B/2':  DiT_B_2,   'DiT-B/4':  DiT_B_4,   'DiT-B/8':  DiT_B_8,\n",
        "    'DiT-S/2':  DiT_S_2,   'DiT-S/4':  DiT_S_4,   'DiT-S/8':  DiT_S_8,\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "4b73dc50",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing train_x2_finetune.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile train_x2_finetune.py\n",
        "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
        "# All rights reserved.\n",
        "\n",
        "# This source code is licensed under the license found in the\n",
        "# LICENSE file in the root directory of this source tree.\n",
        "\n",
        "\"\"\"\n",
        "A minimal training script for DiT using PyTorch DDP.\n",
        "Modified for Fine-tuning ONLY the x2 block on a subset of classes.\n",
        "\"\"\"\n",
        "import torch\n",
        "# the first flag below was False when we tested this script but True makes A100 training a lot faster:\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "import torch.distributed as dist\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "from collections import OrderedDict\n",
        "from PIL import Image\n",
        "from copy import deepcopy\n",
        "from glob import glob\n",
        "from time import time\n",
        "import argparse\n",
        "import logging\n",
        "import os\n",
        "\n",
        "from models import DiT_models\n",
        "from diffusion import create_diffusion\n",
        "from diffusers.models import AutoencoderKL\n",
        "\n",
        "\n",
        "#################################################################################\n",
        "#                             Training Helper Functions                         #\n",
        "#################################################################################\n",
        "\n",
        "@torch.no_grad()\n",
        "def update_ema(ema_model, model, decay=0.9999):\n",
        "    \"\"\"\n",
        "    Step the EMA model towards the current model.\n",
        "    \"\"\"\n",
        "    ema_params = OrderedDict(ema_model.named_parameters())\n",
        "    model_params = OrderedDict(model.named_parameters())\n",
        "\n",
        "    for name, param in model_params.items():\n",
        "        # TODO: Consider applying only to params that require_grad to avoid small numerical changes of pos_embed\n",
        "        ema_params[name].mul_(decay).add_(param.data, alpha=1 - decay)\n",
        "\n",
        "\n",
        "def requires_grad(model, flag=True):\n",
        "    \"\"\"\n",
        "    Set requires_grad flag for all parameters in a model.\n",
        "    \"\"\"\n",
        "    for p in model.parameters():\n",
        "        p.requires_grad = flag\n",
        "\n",
        "\n",
        "def cleanup():\n",
        "    \"\"\"\n",
        "    End DDP training.\n",
        "    \"\"\"\n",
        "    dist.destroy_process_group()\n",
        "\n",
        "\n",
        "def create_logger(logging_dir):\n",
        "    \"\"\"\n",
        "    Create a logger that writes to a log file and stdout.\n",
        "    \"\"\"\n",
        "    if dist.get_rank() == 0:  # real logger\n",
        "        logging.basicConfig(\n",
        "            level=logging.INFO,\n",
        "            format='[\\033[34m%(asctime)s\\033[0m] %(message)s',\n",
        "            datefmt='%Y-%m-%d %H:%M:%S',\n",
        "            handlers=[logging.StreamHandler(), logging.FileHandler(f\"{logging_dir}/log.txt\")]\n",
        "        )\n",
        "        logger = logging.getLogger(__name__)\n",
        "    else:  # dummy logger (does nothing)\n",
        "        logger = logging.getLogger(__name__)\n",
        "        logger.addHandler(logging.NullHandler())\n",
        "    return logger\n",
        "\n",
        "\n",
        "def center_crop_arr(pil_image, image_size):\n",
        "    \"\"\"\n",
        "    Center cropping implementation from ADM.\n",
        "    https://github.com/openai/guided-diffusion/blob/8fb3ad9197f16bbc40620447b2742e13458d2831/guided_diffusion/image_datasets.py#L126\n",
        "    \"\"\"\n",
        "    while min(*pil_image.size) >= 2 * image_size:\n",
        "        pil_image = pil_image.resize(\n",
        "            tuple(x // 2 for x in pil_image.size), resample=Image.BOX\n",
        "        )\n",
        "\n",
        "    scale = image_size / min(*pil_image.size)\n",
        "    pil_image = pil_image.resize(\n",
        "        tuple(round(x * scale) for x in pil_image.size), resample=Image.BICUBIC\n",
        "    )\n",
        "\n",
        "    arr = np.array(pil_image)\n",
        "    crop_y = (arr.shape[0] - image_size) // 2\n",
        "    crop_x = (arr.shape[1] - image_size) // 2\n",
        "    return Image.fromarray(arr[crop_y: crop_y + image_size, crop_x: crop_x + image_size])\n",
        "\n",
        "\n",
        "#################################################################################\n",
        "#                                  Training Loop                                #\n",
        "#################################################################################\n",
        "\n",
        "def main(args):\n",
        "    \"\"\"\n",
        "    Trains a new DiT model.\n",
        "    \"\"\"\n",
        "    assert torch.cuda.is_available(), \"Training currently requires at least one GPU.\"\n",
        "\n",
        "    # Setup DDP:\n",
        "    dist.init_process_group(\"nccl\")\n",
        "    assert args.global_batch_size % dist.get_world_size() == 0, f\"Batch size must be divisible by world size.\"\n",
        "    rank = dist.get_rank()\n",
        "    device = rank % torch.cuda.device_count()\n",
        "    seed = args.global_seed * dist.get_world_size() + rank\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.set_device(device)\n",
        "    print(f\"Starting rank={rank}, seed={seed}, world_size={dist.get_world_size()}.\")\n",
        "\n",
        "    # Setup an experiment folder:\n",
        "    if rank == 0:\n",
        "        os.makedirs(args.results_dir, exist_ok=True)  # Make results folder (holds all experiment subfolders)\n",
        "        experiment_index = len(glob(f\"{args.results_dir}/*\"))\n",
        "        model_string_name = args.model.replace(\"/\", \"-\")  # e.g., DiT-XL/2 --> DiT-XL-2 (for naming folders)\n",
        "        experiment_dir = f\"{args.results_dir}/{experiment_index:03d}-{model_string_name}-x2-finetune\"  # Create an experiment folder\n",
        "        checkpoint_dir = f\"{experiment_dir}/checkpoints\"  # Stores saved model checkpoints\n",
        "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "        logger = create_logger(experiment_dir)\n",
        "        logger.info(f\"Experiment directory created at {experiment_dir}\")\n",
        "    else:\n",
        "        logger = create_logger(None)\n",
        "\n",
        "    # Create model:\n",
        "    assert args.image_size % 8 == 0, \"Image size must be divisible by 8 (for the VAE encoder).\"\n",
        "    latent_size = args.image_size // 8\n",
        "    model = DiT_models[args.model](\n",
        "        input_size=latent_size,\n",
        "        num_classes=args.num_classes\n",
        "    )\n",
        "    # Note that parameter initialization is done within the DiT constructor\n",
        "    \n",
        "    # --- FREEZING LOGIC START ---\n",
        "    logger.info(\"Freezing main DiT model...\")\n",
        "    for p in model.parameters():\n",
        "        p.requires_grad = False\n",
        "    \n",
        "    logger.info(\"Unfreezing x2 components...\")\n",
        "    # Unfreeze x2_embedder\n",
        "    for p in model.x2_embedder.parameters():\n",
        "        p.requires_grad = True\n",
        "    \n",
        "    # Unfreeze x2_vit_block\n",
        "    for p in model.x2_vit_block.parameters():\n",
        "        p.requires_grad = True\n",
        "        \n",
        "    # Unfreeze projections if they exist\n",
        "    if model.x2_vit_proj_in is not None:\n",
        "        logger.info(\"Unfreezing x2_vit_proj_in...\")\n",
        "        for p in model.x2_vit_proj_in.parameters():\n",
        "            p.requires_grad = True\n",
        "            \n",
        "    if model.x2_vit_proj_out is not None:\n",
        "        logger.info(\"Unfreezing x2_vit_proj_out...\")\n",
        "        for p in model.x2_vit_proj_out.parameters():\n",
        "            p.requires_grad = True\n",
        "            \n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    logger.info(f\"Trainable Parameters: {trainable_params:,} / {total_params:,} ({trainable_params/total_params:.2%})\")\n",
        "    # --- FREEZING LOGIC END ---\n",
        "\n",
        "    ema = deepcopy(model).to(device)  # Create an EMA of the model for use after training\n",
        "    requires_grad(ema, False)\n",
        "    model = DDP(model.to(device), device_ids=[rank])\n",
        "    diffusion = create_diffusion(timestep_respacing=\"\")  # default: 1000 steps, linear noise schedule\n",
        "    vae = AutoencoderKL.from_pretrained(f\"stabilityai/sd-vae-ft-{args.vae}\").to(device)\n",
        "\n",
        "    # Setup optimizer (we used default Adam betas=(0.9, 0.999) and a constant learning rate of 1e-4 in our paper):\n",
        "    # Only pass trainable parameters to the optimizer\n",
        "    opt = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4, weight_decay=0)\n",
        "\n",
        "    # Setup data:\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Lambda(lambda pil_image: center_crop_arr(pil_image, args.image_size)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], inplace=True)\n",
        "    ])\n",
        "    \n",
        "    # --- DATASET FILTERING START ---\n",
        "    full_dataset = ImageFolder(args.data_path, transform=transform)\n",
        "    \n",
        "    # Filter dataset for selected classes\n",
        "    logger.info(f\"Filtering dataset for classes: {args.classes}\")\n",
        "    selected_indices = [i for i, label in enumerate(full_dataset.targets) if label in args.classes]\n",
        "    \n",
        "    if len(selected_indices) == 0:\n",
        "        raise ValueError(f\"No images found for classes {args.classes}. Check your dataset or class indices.\")\n",
        "        \n",
        "    dataset = Subset(full_dataset, selected_indices)\n",
        "    logger.info(f\"Filtered Dataset contains {len(dataset):,} images (from {len(full_dataset)} total)\")\n",
        "    # --- DATASET FILTERING END ---\n",
        "\n",
        "    sampler = DistributedSampler(\n",
        "        dataset,\n",
        "        num_replicas=dist.get_world_size(),\n",
        "        rank=rank,\n",
        "        shuffle=True,\n",
        "        seed=args.global_seed\n",
        "    )\n",
        "    loader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=int(args.global_batch_size // dist.get_world_size()),\n",
        "        shuffle=False,\n",
        "        sampler=sampler,\n",
        "        num_workers=args.num_workers,\n",
        "        pin_memory=True,\n",
        "        drop_last=True\n",
        "    )\n",
        "\n",
        "    # Prepare models for training:\n",
        "    update_ema(ema, model.module, decay=0)  # Ensure EMA is initialized with synced weights\n",
        "    model.train()  # important! This enables embedding dropout for classifier-free guidance\n",
        "    ema.eval()  # EMA model should always be in eval mode\n",
        "\n",
        "    # Variables for monitoring/logging purposes:\n",
        "    train_steps = 0\n",
        "    log_steps = 0\n",
        "    running_loss = 0\n",
        "    start_time = time()\n",
        "\n",
        "    logger.info(f\"Training for {args.epochs} epochs...\")\n",
        "    for epoch in range(args.epochs):\n",
        "        sampler.set_epoch(epoch)\n",
        "        logger.info(f\"Beginning epoch {epoch}...\")\n",
        "        for x, y in loader:\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "            with torch.no_grad():\n",
        "                # Map input images to latent space + normalize latents:\n",
        "                x = vae.encode(x).latent_dist.sample().mul_(0.18215)\n",
        "            t = torch.randint(0, diffusion.num_timesteps, (x.shape[0],), device=device)\n",
        "            model_kwargs = dict(y=y)\n",
        "            loss_dict = diffusion.training_losses(model, x, t, model_kwargs)\n",
        "            loss = loss_dict[\"loss\"].mean()\n",
        "            opt.zero_grad()\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            update_ema(ema, model.module)\n",
        "\n",
        "            # Log loss values:\n",
        "            running_loss += loss.item()\n",
        "            log_steps += 1\n",
        "            train_steps += 1\n",
        "            if train_steps % args.log_every == 0:\n",
        "                # Measure training speed:\n",
        "                torch.cuda.synchronize()\n",
        "                end_time = time()\n",
        "                steps_per_sec = log_steps / (end_time - start_time)\n",
        "                # Reduce loss history over all processes:\n",
        "                avg_loss = torch.tensor(running_loss / log_steps, device=device)\n",
        "                dist.all_reduce(avg_loss, op=dist.ReduceOp.SUM)\n",
        "                avg_loss = avg_loss.item() / dist.get_world_size()\n",
        "                logger.info(f\"(step={train_steps:07d}) Train Loss: {avg_loss:.4f}, Train Steps/Sec: {steps_per_sec:.2f}\")\n",
        "                # Reset monitoring variables:\n",
        "                running_loss = 0\n",
        "                log_steps = 0\n",
        "                start_time = time()\n",
        "\n",
        "            # Save DiT checkpoint:\n",
        "            if train_steps % args.ckpt_every == 0 and train_steps > 0:\n",
        "                if rank == 0:\n",
        "                    # Only save weights that were trainable to save space\n",
        "                    trainable_keys = [k for k, p in model.module.named_parameters() if p.requires_grad]\n",
        "                    model_state = {k: v for k, v in model.module.state_dict().items() if k in trainable_keys}\n",
        "                    \n",
        "                    checkpoint = {\n",
        "                        \"model\": model_state, \n",
        "                        # We save full EMA for now to be safe, or we could also just save partial EMA if needed\n",
        "                        # But EMA usually keeps track of everything. To be safe for inference, let's keep full EMA or just trainable parts.\n",
        "                        # For simplicity in this specialized script, let's stick to full EMA so inference scripts don't break,\n",
        "                        # UNLESS the user explicitly wants lightweight.\n",
        "                        # Given the user asked for LoRA-like \"efficient\" storage, best to save only what changed.\n",
        "                        # But standard inference scripts expect full state dict. \n",
        "                        # Compromise: Save full EMA (for immediate use) but partial model (for resume/space).\n",
        "                        \"ema\": ema.state_dict(),\n",
        "                        \"opt\": opt.state_dict(),\n",
        "                        \"args\": args,\n",
        "                        \"x2_finetune_only\": True\n",
        "                    }\n",
        "                    checkpoint_path = f\"{checkpoint_dir}/{train_steps:07d}.pt\"\n",
        "                    torch.save(checkpoint, checkpoint_path)\n",
        "                    logger.info(f\"Saved checkpoint to {checkpoint_path} (Model contains only trainable params)\")\n",
        "                dist.barrier()\n",
        "\n",
        "    model.eval()  # important! This disables randomized embedding dropout\n",
        "    # do any sampling/FID calculation/etc. with ema (or model) in eval mode ...\n",
        "\n",
        "    logger.info(\"Done!\")\n",
        "    cleanup()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Default args here will train DiT-XL/2 with the hyperparameters we used in our paper (except training iters).\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--data-path\", type=str, required=True)\n",
        "    parser.add_argument(\"--results-dir\", type=str, default=\"results\")\n",
        "    parser.add_argument(\"--model\", type=str, choices=list(DiT_models.keys()), default=\"DiT-XL/2\")\n",
        "    parser.add_argument(\"--image-size\", type=int, choices=[256, 512], default=256)\n",
        "    parser.add_argument(\"--num-classes\", type=int, default=1000)\n",
        "    parser.add_argument(\"--epochs\", type=int, default=1400)\n",
        "    parser.add_argument(\"--global-batch-size\", type=int, default=256)\n",
        "    parser.add_argument(\"--global-seed\", type=int, default=0)\n",
        "    parser.add_argument(\"--vae\", type=str, choices=[\"ema\", \"mse\"], default=\"ema\")  # Choice doesn't affect training\n",
        "    parser.add_argument(\"--num-workers\", type=int, default=4)\n",
        "    parser.add_argument(\"--log-every\", type=int, default=100)\n",
        "    parser.add_argument(\"--ckpt-every\", type=int, default=50_000)\n",
        "    \n",
        "    # New arguments\n",
        "    parser.add_argument(\"--classes\", type=int, nargs=\"+\", default=list(range(10)), help=\"List of ImageNet class indices to train on (default: 0-9)\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    main(args)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "7d94379b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing test_freezing.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile test_freezing.py\n",
        "\n",
        "import torch\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Add repo root to path\n",
        "sys.path.append(os.getcwd())\n",
        "\n",
        "from models import DiT_models\n",
        "\n",
        "def verify_freezing():\n",
        "    print(\"Initializing model...\")\n",
        "    model = DiT_models['DiT-XL/2'](\n",
        "        input_size=32,\n",
        "        num_classes=1000\n",
        "    )\n",
        "    \n",
        "    print(\"Applying freezing logic...\")\n",
        "    # --- FREEZING LOGIC COPIED FROM SCRIPT ---\n",
        "    for p in model.parameters():\n",
        "        p.requires_grad = False\n",
        "    \n",
        "    # Check x2_embedder\n",
        "    for p in model.x2_embedder.parameters():\n",
        "        p.requires_grad = True\n",
        "    \n",
        "    # Check x2_vit_block\n",
        "    for p in model.x2_vit_block.parameters():\n",
        "        p.requires_grad = True\n",
        "        \n",
        "    if model.x2_vit_proj_in is not None:\n",
        "        for p in model.x2_vit_proj_in.parameters():\n",
        "            p.requires_grad = True\n",
        "            \n",
        "    if model.x2_vit_proj_out is not None:\n",
        "        for p in model.x2_vit_proj_out.parameters():\n",
        "            p.requires_grad = True\n",
        "    # -----------------------------------------\n",
        "\n",
        "    print(\"Verifying parameters...\")\n",
        "    \n",
        "    # 1. Verify backbone is frozen\n",
        "    frozen_params = [\n",
        "        model.x_embedder.proj.weight,\n",
        "        model.t_embedder.mlp[0].weight,\n",
        "        model.blocks[0].attn.qkv.weight,\n",
        "        model.final_layer.linear.weight\n",
        "    ]\n",
        "    for p in frozen_params:\n",
        "        assert not p.requires_grad, f\"Backbone parameter {p.shape} should be frozen!\"\n",
        "        \n",
        "    # 2. Verify x2 branch is unfrozen\n",
        "    unfrozen_params = [\n",
        "        model.x2_embedder.proj.weight,\n",
        "        model.x2_vit_block.norm1.weight\n",
        "    ]\n",
        "    if model.x2_vit_proj_in is not None:\n",
        "        unfrozen_params.append(model.x2_vit_proj_in.weight)\n",
        "\n",
        "    for p in unfrozen_params:\n",
        "        assert p.requires_grad, f\"x2 parameter {p.shape} should be unfrozen!\"\n",
        "        \n",
        "    print(\"SUCCESS: Freezing logic verified correctly.\")\n",
        "    \n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\"Total Params: {total_params:,}\")\n",
        "    print(f\"Trainable Params: {trainable_params:,} ({trainable_params/total_params:.2%})\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    verify_freezing()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "6e71e5a9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing model...\n",
            "Creating DiT-XL/2 model...\n",
            "[DiT] Loading pre-trained ViT model: vit_large_patch16_224\n",
            "[DiT] Pre-trained ViT block: dim=1024, num_heads=16\n",
            "[DiT] Target x2_vit_block: dim=1152, num_heads=16\n",
            "[DiT] Dimensions don't match. Creating block with pretrained dimensions and projection layers...\n",
            "[DiT] ✓ Successfully loaded pre-trained ViT weights into block with dim=1024\n",
            "[DiT] Using projection layers to adapt dimensions: 1152 -> 1024 -> 1152\n",
            "Applying freezing logic...\n",
            "Verifying parameters...\n",
            "SUCCESS: Freezing logic verified correctly.\n",
            "Total Params: 690,162,208\n",
            "Trainable Params: 15,032,576 (2.18%)\n"
          ]
        }
      ],
      "source": [
        "# Verify that the freezing logic works correctly\n",
        "!python test_freezing.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "5a17d795",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading ImageNet validation set (~6.3GB)...\n",
            "--2025-12-07 20:14:12--  https://image-net.org/data/ILSVRC/2012/ILSVRC2012_img_val.tar\n",
            "Resolving image-net.org (image-net.org)... 171.64.68.16\n",
            "Connecting to image-net.org (image-net.org)|171.64.68.16|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6744924160 (6.3G) [application/x-tar]\n",
            "Saving to: ‘ILSVRC2012_img_val.tar’\n",
            "\n",
            "ILSVRC2012_img_val. 100%[===================>]   6.28G  23.6MB/s    in 4m 41s  \n",
            "\n",
            "2025-12-07 20:18:53 (22.9 MB/s) - ‘ILSVRC2012_img_val.tar’ saved [6744924160/6744924160]\n",
            "\n",
            "Downloading validation ground truth...\n",
            "--2025-12-07 20:18:53--  https://image-net.org/data/ILSVRC/2012/ILSVRC2012_devkit_t12.tar.gz\n",
            "Resolving image-net.org (image-net.org)... 171.64.68.16\n",
            "Connecting to image-net.org (image-net.org)|171.64.68.16|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2568145 (2.4M) [application/x-gzip]\n",
            "Saving to: ‘ILSVRC2012_devkit_t12.tar.gz’\n",
            "\n",
            "ILSVRC2012_devkit_t 100%[===================>]   2.45M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2025-12-07 20:18:54 (18.3 MB/s) - ‘ILSVRC2012_devkit_t12.tar.gz’ saved [2568145/2568145]\n",
            "\n",
            "Extracting validation images...\n",
            "Extracting devkit...\n",
            "Parsing validation ground truth...\n",
            "Organizing into class folders...\n",
            "✓ ImageNet validation set organized into 1000 class folders\n",
            "Total images: 50000\n",
            "Dataset ready at: ./imagenet_val_organized\n"
          ]
        }
      ],
      "source": [
        "# Download ImageNet Validation Dataset (ILSVRC2012)\n",
        "# This will download ~6.3GB and organize it into class folders\n",
        "\n",
        "import os\n",
        "import tarfile\n",
        "from pathlib import Path\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "# Download validation set\n",
        "print('Downloading ImageNet validation set (~6.3GB)...')\n",
        "!wget -nc https://image-net.org/data/ILSVRC/2012/ILSVRC2012_img_val.tar\n",
        "\n",
        "# Download validation ground truth annotations\n",
        "print('Downloading validation ground truth...')\n",
        "!wget -nc https://image-net.org/data/ILSVRC/2012/ILSVRC2012_devkit_t12.tar.gz\n",
        "\n",
        "# Extract validation images\n",
        "print('Extracting validation images...')\n",
        "os.makedirs('imagenet_val', exist_ok=True)\n",
        "!tar -xf ILSVRC2012_img_val.tar -C imagenet_val\n",
        "\n",
        "# Extract devkit to get ground truth\n",
        "print('Extracting devkit...')\n",
        "!tar -xzf ILSVRC2012_devkit_t12.tar.gz\n",
        "\n",
        "# Parse ground truth from devkit\n",
        "print('Parsing validation ground truth...')\n",
        "gt_file = 'ILSVRC2012_devkit_t12/data/ILSVRC2012_validation_ground_truth.txt'\n",
        "with open(gt_file, 'r') as f:\n",
        "    # Ground truth file has 1-indexed class IDs, convert to 0-indexed\n",
        "    labels = [int(line.strip()) - 1 for line in f]\n",
        "\n",
        "# Organize into class folders\n",
        "print('Organizing into class folders...')\n",
        "val_dir = Path('imagenet_val')\n",
        "organized_dir = Path('imagenet_val_organized')\n",
        "\n",
        "# Create class directories\n",
        "for class_id in set(labels):\n",
        "    (organized_dir / str(class_id)).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Move images to class folders\n",
        "val_images = sorted(val_dir.glob('ILSVRC2012_val_*.JPEG'))\n",
        "for idx, img_path in enumerate(val_images):\n",
        "    class_id = labels[idx]\n",
        "    target_path = organized_dir / str(class_id) / img_path.name\n",
        "    img_path.rename(target_path)\n",
        "\n",
        "print(f'✓ ImageNet validation set organized into {len(set(labels))} class folders')\n",
        "print(f'Total images: {len(val_images)}')\n",
        "print('Dataset ready at: ./imagenet_val_organized')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "7e4de794",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/backends/__init__.py:46: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
            "  self.setter(val)\n",
            "2025-12-07 20:34:12.797882: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1765139652.819410    7704 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1765139652.825587    7704 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1765139652.840981    7704 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765139652.841006    7704 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765139652.841010    7704 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765139652.841015    7704 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-07 20:34:12.845793: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Starting rank=0, seed=0, world_size=1.\n",
            "Creating DiT-XL/2 model...\n",
            "[DiT] Loading pre-trained ViT model: vit_large_patch16_224\n",
            "[DiT] Pre-trained ViT block: dim=1024, num_heads=16\n",
            "[DiT] Target x2_vit_block: dim=1152, num_heads=16\n",
            "[DiT] Dimensions don't match. Creating block with pretrained dimensions and projection layers...\n",
            "[DiT] ✓ Successfully loaded pre-trained ViT weights into block with dim=1024\n",
            "[DiT] Using projection layers to adapt dimensions: 1152 -> 1024 -> 1152\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [164-943], Classes: [972, 974, 973, 972]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [315-733], Classes: [973, 974, 975, 972]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [416-819], Classes: [972, 975, 974, 972]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [103-917], Classes: [973, 972, 973, 972]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [766-867], Classes: [972, 972, 974, 976]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [27-950], Classes: [976, 976, 973, 973]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [95-853], Classes: [976, 973, 976, 974]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [198-835], Classes: [973, 972, 975, 973]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [127-885], Classes: [974, 976, 975, 973]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [120-781], Classes: [976, 972, 974, 976]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [50-904], Classes: [975, 974, 973, 974]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [285-978], Classes: [974, 972, 976, 974]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [143-560], Classes: [972, 976, 973, 976]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [307-980], Classes: [973, 972, 973, 973]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [141-602], Classes: [975, 976, 975, 972]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [258-946], Classes: [975, 975, 974, 976]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [260-949], Classes: [975, 975, 973, 972]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [274-645], Classes: [974, 975, 973, 974]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [102-725], Classes: [972, 973, 973, 975]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [127-911], Classes: [972, 974, 975, 975]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [95-739], Classes: [974, 972, 976, 976]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [282-853], Classes: [973, 976, 972, 973]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [167-777], Classes: [973, 972, 973, 975]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [35-942], Classes: [972, 973, 975, 972]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [10-563], Classes: [976, 976, 972, 972]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [163-862], Classes: [974, 972, 973, 976]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [274-808], Classes: [975, 975, 972, 974]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [35-791], Classes: [975, 976, 974, 973]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [232-840], Classes: [975, 975, 975, 975]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [136-895], Classes: [975, 973, 972, 975]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [293-921], Classes: [975, 976, 974, 973]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [151-823], Classes: [975, 974, 976, 976]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [34-516], Classes: [973, 976, 973, 974]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [58-775], Classes: [974, 974, 973, 975]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [83-809], Classes: [974, 973, 973, 974]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [114-502], Classes: [975, 972, 976, 976]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [127-997], Classes: [974, 974, 972, 974]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [60-369], Classes: [972, 974, 973, 974]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [162-642], Classes: [976, 972, 972, 974]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [40-779], Classes: [972, 973, 975, 972]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [207-660], Classes: [975, 974, 974, 976]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [233-950], Classes: [974, 974, 975, 972]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [239-896], Classes: [974, 973, 976, 976]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [103-847], Classes: [974, 976, 973, 972]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [81-639], Classes: [976, 975, 976, 976]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [22-993], Classes: [975, 974, 974, 976]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [24-625], Classes: [976, 972, 974, 973]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [67-858], Classes: [974, 972, 975, 976]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [225-866], Classes: [976, 974, 973, 974]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [11-552], Classes: [975, 976, 976, 975]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [262-895], Classes: [973, 975, 975, 972]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [192-789], Classes: [974, 976, 975, 972]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [113-981], Classes: [975, 975, 973, 973]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [391-869], Classes: [972, 976, 975, 976]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [48-858], Classes: [973, 972, 973, 973]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [140-985], Classes: [974, 972, 974, 976]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [194-974], Classes: [975, 976, 973, 976]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [210-952], Classes: [972, 973, 975, 975]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [65-628], Classes: [974, 975, 973, 976]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [31-809], Classes: [972, 975, 973, 972]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [165-550], Classes: [972, 974, 972, 975]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [18-840], Classes: [976, 973, 972, 976]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [457-768], Classes: [973, 975, 974, 972]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [224-690], Classes: [972, 976, 972, 976]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [202-965], Classes: [972, 975, 974, 974]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [210-899], Classes: [976, 975, 972, 974]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [125-919], Classes: [975, 972, 972, 974]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [562-956], Classes: [974, 975, 973, 974]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [155-940], Classes: [976, 975, 975, 973]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [358-884], Classes: [975, 972, 975, 976]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [83-985], Classes: [975, 973, 976, 975]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [429-857], Classes: [974, 974, 974, 975]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [101-895], Classes: [973, 973, 973, 972]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [234-906], Classes: [972, 972, 973, 973]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [217-802], Classes: [976, 972, 976, 975]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [73-856], Classes: [973, 973, 974, 976]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [231-953], Classes: [972, 974, 972, 972]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [504-988], Classes: [972, 976, 976, 974]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [302-875], Classes: [973, 975, 973, 974]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [144-844], Classes: [972, 973, 973, 974]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [442-916], Classes: [972, 972, 972, 972]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [211-939], Classes: [976, 973, 975, 974]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [308-920], Classes: [974, 974, 975, 973]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [260-733], Classes: [975, 972, 974, 974]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [5-988], Classes: [976, 975, 974, 972]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [17-948], Classes: [974, 975, 973, 972]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [195-778], Classes: [973, 975, 975, 973]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [70-314], Classes: [975, 973, 975, 975]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [48-998], Classes: [976, 973, 974, 974]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [35-942], Classes: [976, 972, 973, 972]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [7-60], Classes: [975, 976, 975, 974]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [167-946], Classes: [973, 973, 973, 976]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [101-693], Classes: [975, 973, 975, 976]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [206-586], Classes: [972, 976, 974, 976]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [71-893], Classes: [973, 972, 973, 973]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [14-968], Classes: [975, 973, 976, 972]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [430-818], Classes: [976, 976, 974, 974]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [156-507], Classes: [973, 974, 976, 973]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [20-798], Classes: [972, 975, 976, 974]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [222-620], Classes: [972, 976, 972, 973]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [160-676], Classes: [974, 975, 975, 975]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [252-552], Classes: [974, 974, 974, 975]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [223-761], Classes: [976, 975, 975, 976]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [418-924], Classes: [973, 976, 976, 973]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [68-350], Classes: [972, 972, 976, 972]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [48-628], Classes: [973, 973, 976, 976]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [252-917], Classes: [975, 974, 975, 973]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [338-948], Classes: [975, 974, 974, 976]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [17-443], Classes: [973, 972, 972, 972]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [13-694], Classes: [974, 974, 972, 973]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [218-950], Classes: [972, 975, 976, 976]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [250-760], Classes: [976, 972, 975, 972]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [16-986], Classes: [974, 973, 975, 973]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [111-953], Classes: [973, 972, 976, 976]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [7-984], Classes: [975, 974, 975, 976]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [13-408], Classes: [975, 975, 973, 976]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [174-567], Classes: [976, 972, 974, 976]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [43-628], Classes: [975, 972, 976, 974]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [620-864], Classes: [972, 973, 976, 973]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [166-939], Classes: [976, 972, 974, 972]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [101-932], Classes: [973, 976, 974, 976]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [113-790], Classes: [974, 976, 976, 972]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [259-914], Classes: [973, 974, 975, 974]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] Batch: torch.Size([4, 4, 32, 32]), Timesteps: [154-902], Classes: [972, 975, 973, 975]\n",
            "[DiT Forward] x2 after transpose: torch.Size([4, 1152, 64])\n",
            "[DiT Forward] x2 after interpolate: torch.Size([4, 1152, 256])\n",
            "[DiT Forward] x2 after transpose back: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([4, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([4, 256, 1152])\n"
          ]
        }
      ],
      "source": [
        "# Run Fine-Tuning\n",
        "# Using ImageNet validation set organized above\n",
        "!torchrun --nnodes=1 --nproc_per_node=1 train_x2_finetune.py \\\n",
        "    --model DiT-XL/2 \\\n",
        "    --data-path ./imagenet_val_organized \\\n",
        "    --classes 972 973 974 975 976 \\\n",
        "    --epochs 2 \\\n",
        "    --global-batch-size 4 \\\n",
        "    --log-every 1   # Changed from 10 to 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "1e317db7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "checkpoints  log.txt\n"
          ]
        }
      ],
      "source": [
        "# # Check if checkpoints exist\n",
        "# !ls  results/*/checkpoints/\n",
        "\n",
        "# # Check training logs\n",
        "!cat results/002-DiT-XL-2-x2-finetune/log.txt\n",
        "!ls results/002-DiT-XL-2-x2-finetune/"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
