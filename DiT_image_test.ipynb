{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DiT Image Generation Test Notebook\n",
        "\n",
        "Use this notebook to quickly verify recent model changes (e.g., newly added residual connections) by sampling images from a DiT checkpoint.\n",
        "\n",
        "**Important for Colab users:** This notebook clones the original DiT repo from GitHub. To test your local modifications:\n",
        "1. Upload your modified `models.py` to Colab after running Cell 1, OR\n",
        "2. Run this notebook locally with Jupyter instead\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running in: Google Colab\n",
            "DiT repository exists. Pulling latest changes...\n",
            "Your branch is up to date with 'origin/wip/duplicate-x-difference-embedding'.\n",
            "\n",
            "✓ Successfully updated DiT repository\n",
            "Installing dependencies...\n",
            "Repository root: /content/DiT\n",
            "Current working directory: /content/DiT\n",
            "✓ Repository root found successfully\n",
            "Using device: cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/backends/__init__.py:46: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
            "  self.setter(val)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import shutil\n",
        "\n",
        "# Detect environment and setup paths\n",
        "in_colab = 'google.colab' in sys.modules\n",
        "print(f\"Running in: {'Google Colab' if in_colab else 'Local environment'}\")\n",
        "\n",
        "if in_colab:\n",
        "    # Update or clone the repo\n",
        "    import subprocess\n",
        "    \n",
        "    if os.path.exists('/content/DiT'):\n",
        "        print(\"DiT repository exists. Pulling latest changes...\")\n",
        "        os.chdir('/content/DiT')\n",
        "        result = subprocess.run(['git', 'pull' , 'origin', 'wip/duplicate-x-difference-embedding'], cwd='/content/DiT', capture_output=True, text=True)\n",
        "        result = subprocess.run(['git', 'checkout', 'wip/duplicate-x-difference-embedding'], cwd='/content/DiT', capture_output=True, text=True)\n",
        "        print(result.stdout)\n",
        "        if result.returncode == 0:\n",
        "            print(\"✓ Successfully updated DiT repository\")\n",
        "        else:\n",
        "            print(f\"⚠️ Pull failed: {result.stderr}\")\n",
        "    else:\n",
        "        print(\"DiT repository not found. Cloning...\")\n",
        "        result = subprocess.run(['git', 'clone', '-b', 'wip/duplicate-x-difference-embedding', 'https://github.com/mrmc-speco/DiT.git'], cwd='/content', capture_output=True, text=True)\n",
        "        print(result.stdout)\n",
        "        if result.returncode == 0:\n",
        "            print(\"✓ Successfully cloned DiT repository\")\n",
        "        else:\n",
        "            print(f\"⚠️ Clone failed: {result.stderr}\")\n",
        "    \n",
        "    # Change to DiT directory\n",
        "    os.chdir('/content/DiT')\n",
        "    \n",
        "    # Install dependencies\n",
        "    print(\"Installing dependencies...\")\n",
        "    import subprocess\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', '-q', 'diffusers', 'timm'])\n",
        "    \n",
        "    repo_root = '/content/DiT'\n",
        "else:\n",
        "    # Local environment - assume we're in the DiT directory\n",
        "    repo_root = os.getcwd()\n",
        "    # If models.py is not in current dir, try parent\n",
        "    if not os.path.exists(os.path.join(repo_root, 'models.py')):\n",
        "        repo_root = os.path.dirname(repo_root)\n",
        "\n",
        "# Add to Python path\n",
        "if repo_root not in sys.path:\n",
        "    sys.path.insert(0, repo_root)\n",
        "\n",
        "print(f\"Repository root: {repo_root}\")\n",
        "print(f\"Current working directory: {os.getcwd()}\")\n",
        "\n",
        "# Verify the path is correct\n",
        "if os.path.exists(os.path.join(repo_root, 'models.py')):\n",
        "    print(\"✓ Repository root found successfully\")\n",
        "else:\n",
        "    print(f\"\\n⚠️  WARNING: models.py not found at {repo_root}\")\n",
        "\n",
        "import torch\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "from diffusion import create_diffusion\n",
        "from diffusers.models import AutoencoderKL\n",
        "from download import find_model\n",
        "from models import DiT_models\n",
        "from PIL import Image\n",
        "from IPython.display import display\n",
        "\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "torch.set_grad_enabled(False)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optional: Upload Modified models.py (Colab only)\n",
        "\n",
        "If you're in Colab and want to test your local modifications, run this cell to upload your modified `models.py` file:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Patch models.py with skip connection (Colab only)\n",
        "# import sys\n",
        "\n",
        "# in_colab = 'google.colab' in sys.modules\n",
        "\n",
        "# if in_colab:\n",
        "#     print(\"Applying skip connection patch to models.py...\")\n",
        "    \n",
        "#     # Use sed-like approach to patch the file\n",
        "#     import subprocess\n",
        "    \n",
        "#     # Create a Python script to patch the file\n",
        "#     patch_script = '''\n",
        "# import re\n",
        "\n",
        "# with open('/content/DiT/models.py', 'r') as f:\n",
        "#     content = f.read()\n",
        "\n",
        "# # Check if already patched\n",
        "# if 'skip = x' in content and 'preserve pre-block representation' in content:\n",
        "#     print(\"✓ Skip connection already applied!\")\n",
        "# else:\n",
        "#     # Patch forward method\n",
        "#     pattern = r\"(def forward\\\\(self, x, t, y\\\\):.*?\\\\\\\"\\\\\\\"\\\\\\\"\\\\n)\"\n",
        "#     replacement = r'\\\\1        # Log forward pass\\\\n        print(f\"[DiT Forward] Batch: {x.shape}, Timesteps: [{t.min().item():.0f}-{t.max().item():.0f}]\")\\\\n        '\n",
        "#     content = re.sub(pattern, replacement, content, flags=re.DOTALL)\n",
        "    \n",
        "#     # Add skip connection\n",
        "#     pattern = r\"(c = t \\\\+ y.*?# \\\\(N, D\\\\)\\\\n)        (for block in self\\\\.blocks:)\"\n",
        "#     replacement = r\"\\\\1        skip = x  # preserve pre-block representation\\\\n        \\\\2\"\n",
        "#     content = re.sub(pattern, replacement, content)\n",
        "    \n",
        "#     pattern = r\"(for block in self\\\\.blocks:.*?x = block\\\\(x, c\\\\).*?\\\\n)        (x = self\\\\.final_layer)\"\n",
        "#     replacement = r\"\\\\1        x = x + skip  # skip connection\\\\n        print(f'[DiT Forward] ✓ Skip applied across {len(self.blocks)} blocks')\\\\n        \\\\2\"\n",
        "#     content = re.sub(pattern, replacement, content, flags=re.DOTALL)\n",
        "    \n",
        "#     with open('/content/DiT/models.py', 'w') as f:\n",
        "#         f.write(content)\n",
        "    \n",
        "#     print(\"✓ Patch applied!\")\n",
        "# '''\n",
        "    \n",
        "#     with open('/tmp/patch.py', 'w') as f:\n",
        "#         f.write(patch_script)\n",
        "    \n",
        "#     result = subprocess.run(['python', '/tmp/patch.py'], capture_output=True, text=True)\n",
        "#     print(result.stdout)\n",
        "    \n",
        "#     if result.returncode == 0:\n",
        "#         print(\"⚠️  Please restart runtime (Runtime > Restart runtime) and re-run all cells\")\n",
        "#     else:\n",
        "#         print(f\"Error: {result.stderr}\")\n",
        "# else:\n",
        "#     print(\"Not in Colab - skipping patch\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading DiT-XL/2 from DiT-XL-2-256x256.pt\n",
            "Creating DiT-XL/2 model...\n",
            "[DiT] Loading pre-trained ViT model: vit_large_patch16_224\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
            "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
            "You are not authenticated with the Hugging Face Hub in this notebook.\n",
            "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[DiT] Pre-trained ViT block: dim=1024, num_heads=16\n",
            "[DiT] Target x2_vit_block: dim=1152, num_heads=16\n",
            "[DiT] Dimensions don't match. Creating block with pretrained dimensions and projection layers...\n",
            "[DiT] ✓ Successfully loaded pre-trained ViT weights into block with dim=1024\n",
            "[DiT] Using projection layers to adapt dimensions: 1152 -> 1024 -> 1152\n",
            "⚠️  Warning: Missing keys in checkpoint (will use random initialization):\n",
            "   - x2_embedder.proj.weight\n",
            "   - x2_embedder.proj.bias\n",
            "   - x2_vit_block.norm1.weight\n",
            "   - x2_vit_block.norm1.bias\n",
            "   - x2_vit_block.attn.qkv.weight\n",
            "   - x2_vit_block.attn.qkv.bias\n",
            "   - x2_vit_block.attn.proj.weight\n",
            "   - x2_vit_block.attn.proj.bias\n",
            "   - x2_vit_block.norm2.weight\n",
            "   - x2_vit_block.norm2.bias\n",
            "   - x2_vit_block.mlp.fc1.weight\n",
            "   - x2_vit_block.mlp.fc1.bias\n",
            "   - x2_vit_block.mlp.fc2.weight\n",
            "   - x2_vit_block.mlp.fc2.bias\n",
            "   - x2_vit_proj_in.weight\n",
            "   - x2_vit_proj_in.bias\n",
            "   - x2_vit_proj_out.weight\n",
            "   - x2_vit_proj_out.bias\n"
          ]
        }
      ],
      "source": [
        "# --- Configuration ---------------------------------------------------------\n",
        "model_key = \"DiT-XL/2\"      # Choose from models.DiT_models\n",
        "image_size = 256             # 256 or 512 for auto-downloaded checkpoints\n",
        "vae_variant = \"ema\"          # \"mse\" or \"ema\" (Stable Diffusion VAE variants)\n",
        "num_sampling_steps = 250    # More steps -> better quality, slower\n",
        "cfg_scale = 4.0\n",
        "class_labels = [387,387,387]  # ImageNet class IDs\n",
        "samples_per_row = 3\n",
        "seed = 0\n",
        "custom_ckpt = None          # Optional path to a custom DiT checkpoint\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "latent_size = image_size // 8\n",
        "\n",
        "if custom_ckpt is None:\n",
        "    if model_key != \"DiT-XL/2\":\n",
        "        raise ValueError(\"Automatic checkpoints are only available for DiT-XL/2. Provide custom_ckpt instead.\")\n",
        "    ckpt_path = f\"DiT-XL-2-{image_size}x{image_size}.pt\"\n",
        "else:\n",
        "    ckpt_path = custom_ckpt\n",
        "\n",
        "print(f\"Loading {model_key} from {ckpt_path}\")\n",
        "model = DiT_models[model_key](input_size=latent_size).to(device)\n",
        "state_dict = find_model(ckpt_path)\n",
        "\n",
        "# Load with strict=False to allow missing keys (e.g., new x2_embedder)\n",
        "missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)\n",
        "if missing_keys:\n",
        "    print(f\"⚠️  Warning: Missing keys in checkpoint (will use random initialization):\")\n",
        "    for key in missing_keys:\n",
        "        print(f\"   - {key}\")\n",
        "if unexpected_keys:\n",
        "    print(f\"⚠️  Warning: Unexpected keys in checkpoint:\")\n",
        "    for key in unexpected_keys:\n",
        "        print(f\"   - {key}\")\n",
        "        \n",
        "model.eval()\n",
        "\n",
        "diffusion = create_diffusion(str(num_sampling_steps))\n",
        "vae = AutoencoderKL.from_pretrained(f\"stabilityai/sd-vae-ft-{vae_variant}\").to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DiT(\n",
            "  (x_embedder): PatchEmbed(\n",
            "    (proj): Conv2d(4, 1152, kernel_size=(2, 2), stride=(2, 2))\n",
            "    (norm): Identity()\n",
            "  )\n",
            "  (x2_embedder): PatchEmbed(\n",
            "    (proj): Conv2d(4, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (norm): Identity()\n",
            "  )\n",
            "  (t_embedder): TimestepEmbedder(\n",
            "    (mlp): Sequential(\n",
            "      (0): Linear(in_features=256, out_features=1152, bias=True)\n",
            "      (1): SiLU()\n",
            "      (2): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (y_embedder): LabelEmbedder(\n",
            "    (embedding_table): Embedding(1001, 1152)\n",
            "  )\n",
            "  (blocks): ModuleList(\n",
            "    (0-27): 28 x DiTBlock(\n",
            "      (norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=False)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
            "        (q_norm): Identity()\n",
            "        (k_norm): Identity()\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (norm): Identity()\n",
            "        (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=False)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=1152, out_features=4608, bias=True)\n",
            "        (act): GELU(approximate='tanh')\n",
            "        (drop1): Dropout(p=0, inplace=False)\n",
            "        (norm): Identity()\n",
            "        (fc2): Linear(in_features=4608, out_features=1152, bias=True)\n",
            "        (drop2): Dropout(p=0, inplace=False)\n",
            "      )\n",
            "      (adaLN_modulation): Sequential(\n",
            "        (0): SiLU()\n",
            "        (1): Linear(in_features=1152, out_features=6912, bias=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (x2_vit_block): Block(\n",
            "    (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
            "    (attn): Attention(\n",
            "      (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
            "      (q_norm): Identity()\n",
            "      (k_norm): Identity()\n",
            "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "      (norm): Identity()\n",
            "      (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (ls1): Identity()\n",
            "    (drop_path1): Identity()\n",
            "    (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
            "    (mlp): Mlp(\n",
            "      (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "      (act): GELU(approximate='none')\n",
            "      (drop1): Dropout(p=0.0, inplace=False)\n",
            "      (norm): Identity()\n",
            "      (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "      (drop2): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (ls2): Identity()\n",
            "    (drop_path2): Identity()\n",
            "  )\n",
            "  (final_layer): FinalLayer(\n",
            "    (norm_final): LayerNorm((1152,), eps=1e-06, elementwise_affine=False)\n",
            "    (linear): Linear(in_features=1152, out_features=32, bias=True)\n",
            "    (adaLN_modulation): Sequential(\n",
            "      (0): SiLU()\n",
            "      (1): Linear(in_features=1152, out_features=2304, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (x2_vit_proj_in): Linear(in_features=1152, out_features=1024, bias=True)\n",
            "  (x2_vit_proj_out): Linear(in_features=1024, out_features=1152, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Starting sampling process...\n",
            "Will run 250 diffusion steps\n",
            "Watch for [DiT CFG] and [DiT Forward] log messages below:\n",
            "============================================================\n",
            "\n",
            "\n",
            "============================================================\n",
            "[CFG] Input: torch.Size([6, 4, 32, 32]), CFG scale: 4.0\n",
            "[DiT Forward] Batch: torch.Size([6, 4, 32, 32]), Timesteps: [999-999], Classes: [387, 387, 387, 1000]\n",
            "[DiT Forward] x2 after transpose back: torch.Size([6, 256, 1152])\n",
            "[DiT Forward] x2 after ViT block: torch.Size([6, 256, 1152])\n",
            "[DiT Forward] x: torch.Size([6, 256, 1152])\n",
            "[DiT Forward] x after addition: torch.Size([6, 256, 1152])\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'DiTBlock' object has no attribute 'shape'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3145744658.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m60\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m samples = diffusion.p_sample_loop(\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_with_cfg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/DiT/diffusion/gaussian_diffusion.py\u001b[0m in \u001b[0;36mp_sample_loop\u001b[0;34m(self, model, shape, noise, clip_denoised, denoised_fn, cond_fn, model_kwargs, device, progress)\u001b[0m\n\u001b[1;32m    448\u001b[0m         \"\"\"\n\u001b[1;32m    449\u001b[0m         \u001b[0mfinal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 450\u001b[0;31m         for sample in self.p_sample_loop_progressive(\n\u001b[0m\u001b[1;32m    451\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m             \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/DiT/diffusion/gaussian_diffusion.py\u001b[0m in \u001b[0;36mp_sample_loop_progressive\u001b[0;34m(self, model, shape, noise, clip_denoised, denoised_fn, cond_fn, model_kwargs, device, progress)\u001b[0m\n\u001b[1;32m    499\u001b[0m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 501\u001b[0;31m                 out = self.p_sample(\n\u001b[0m\u001b[1;32m    502\u001b[0m                     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m                     \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/DiT/diffusion/gaussian_diffusion.py\u001b[0m in \u001b[0;36mp_sample\u001b[0;34m(self, model, x, t, clip_denoised, denoised_fn, cond_fn, model_kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m                  \u001b[0;34m-\u001b[0m \u001b[0;34m'pred_xstart'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ma\u001b[0m \u001b[0mprediction\u001b[0m \u001b[0mof\u001b[0m \u001b[0mx_0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m         \"\"\"\n\u001b[0;32m--> 402\u001b[0;31m         out = self.p_mean_variance(\n\u001b[0m\u001b[1;32m    403\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/DiT/diffusion/respace.py\u001b[0m in \u001b[0;36mp_mean_variance\u001b[0;34m(self, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     ):  # pylint: disable=signature-differs\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp_mean_variance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrap_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     def training_losses(\n",
            "\u001b[0;32m/content/DiT/diffusion/gaussian_diffusion.py\u001b[0m in \u001b[0;36mp_mean_variance\u001b[0;34m(self, model, x, t, clip_denoised, denoised_fn, model_kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m         \u001b[0mmodel_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0mmodel_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/DiT/diffusion/respace.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, ts, **kwargs)\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;31m# if self.rescale_timesteps:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;31m#     new_ts = new_ts.float() * (1000.0 / self.original_num_steps)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_ts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/DiT/models.py\u001b[0m in \u001b[0;36mforward_with_cfg\u001b[0;34m(self, x, t, y, cfg_scale)\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0mcombined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhalf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhalf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m         \u001b[0mmodel_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m         \u001b[0;31m# For exact reproducibility reasons, we apply classifier-free guidance on only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/DiT/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, t, y)\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[DiT Forward] x after addition: {x.shape}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflush\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[DiT Forward] block {i}: {block.shape}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflush\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1962\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1964\u001b[0;31m         raise AttributeError(\n\u001b[0m\u001b[1;32m   1965\u001b[0m             \u001b[0;34mf\"'{type(self).__name__}' object has no attribute '{name}'\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1966\u001b[0m         )\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'DiTBlock' object has no attribute 'shape'"
          ]
        }
      ],
      "source": [
        "n = len(class_labels)\n",
        "z = torch.randn(n, 4, latent_size, latent_size, device=device)\n",
        "y = torch.tensor(class_labels, device=device)\n",
        "\n",
        "# Classifier-free guidance setup:\n",
        "z = torch.cat([z, z], dim=0)\n",
        "y_null = torch.tensor([1000] * n, device=device)\n",
        "y = torch.cat([y, y_null], dim=0)\n",
        "model_kwargs = dict(y=y, cfg_scale=cfg_scale)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Starting sampling process...\")\n",
        "print(f\"Will run {num_sampling_steps} diffusion steps\")\n",
        "print(\"Watch for [DiT CFG] and [DiT Forward] log messages below:\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "samples = diffusion.p_sample_loop(\n",
        "    model.forward_with_cfg,\n",
        "    z.shape,\n",
        "    z,\n",
        "    clip_denoised=False,\n",
        "    model_kwargs=model_kwargs,\n",
        "    progress=False,  # Disable tqdm to see our prints clearly\n",
        "    device=device,\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Sampling complete! Decoding with VAE...\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "samples, _ = samples.chunk(2, dim=0)\n",
        "samples = vae.decode(samples / 0.18215).sample\n",
        "\n",
        "output_path = \"dit_test_samples.png\"\n",
        "save_image(samples, output_path, nrow=samples_per_row, normalize=True, value_range=(-1, 1))\n",
        "print(f\"✓ Saved grid to {output_path}\")\n",
        "display(Image.open(output_path))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!ls /content/DiT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "display(Image.open(\"/content/DiT/dit_test_samples.png\"))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
